# -*- coding: utf-8 -*-
"""hw5_implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UUTZaOlRsnLrzs3bbSbxmQkQyJOUR1tj

your name:
Rabee Parhizkari

your student number:
400109413

in case you had any questions you can contact me via telegram: @levi_acker_mann

1-implementation of harris feature detector:

a) First, import both pictures. As a preprocessing step, apply a Gaussian filter to reduce noise. It is also highly recommended to convert the images to grayscale. (In this section, you can use prebuilt libraries.)
"""

import cv2
image1 = cv2.imread('/content/img1.png')
image2 = cv2.imread('/content/img2.png')

blured_image1 = cv2.GaussianBlur(image1,(5,5),0)
blured_image2 = cv2.GaussianBlur(image2,(5,5),0)

gray_image1 = cv2.cvtColor(blured_image1, cv2.COLOR_BGR2GRAY)
gray_image2 = cv2.cvtColor(blured_image2, cv2.COLOR_BGR2GRAY)

"""b) as you know for detecting corners you need to find to image gradients so compute the image gradients in 2 directions. use libraries for this purpose."""

grad_x1 = cv2.Sobel(gray_image1,cv2.CV_64F,1,0,ksize=3)
grad_y1 = cv2.Sobel(gray_image1,cv2.CV_64F,0,1,ksize=3)
grad_x2 = cv2.Sobel(gray_image1,cv2.CV_64F,1,0,ksize=3)
grad_y2 = cv2.Sobel(gray_image1,cv2.CV_64F,0,1,ksize=3)

"""c) Compute the Harris corner response for each pixel. Use an arbitrary value for the window size. You are not allowed to use prebuilt libraries for the Harris corner detector."""

I_x2 = grad_x1*grad_x1
I_y2 = grad_y1*grad_y2
I_xy = grad_x1*grad_y1
I_x2 = cv2.GaussianBlur(I_x2,(3,3),0)
I_y2 = cv2.GaussianBlur(I_y2,(3,3),0)
I_xy = cv2.GaussianBlur(I_xy,(3,3),0)
trace = I_x2 + I_y2
detM = (I_x2 * I_y2) - (I_xy ** 2)

R = detM - 0.05 * (trace ** 2)

"""d)  Identify a set of pixels where the Harris response exceeds a specified, arbitrary threshold value of Œ±."""

new_R = np.copy(R)
cv2_imshow(new_R)
threshold = 50
for i in range(0,R.shape[0]):
  for j in range(0,R.shape[1]):
    if(R[i][j]<threshold):
      new_R[i][j] = 0
    else:
      new_R[i][j] = 255

cv2_imshow(new_R)

"""e) apply non-maximal suppression within an arbitrary w√ów pixel neighborhood around each candidate pixel to extract the most significant corners.

"""

import numpy as np
def non_max_suppression(R, w):
    pad = w // 2
    padded_response = np.pad(R, ((pad, pad), (pad, pad)), mode='constant')
    suppressed_response = np.zeros_like(R)

    for i in range(R.shape[0]):
        for j in range(R.shape[1]):
            block = padded_response[i:i+w, j:j+w]
            max_val = np.max(block)
            if R[i, j] == max_val:
                suppressed_response[i, j] = R[i, j]
            else:
                suppressed_response[i, j] = 255

    return suppressed_response
non_max_suppression = non_max_suppression(new_R,10)
cv2_imshow(non_max_suppression)

"""f) Visualize the input images with highlighted corners"""

# Find the coordinates of the corners
corners = np.argwhere(non_max_suppression > 0)
for corner in corners:
    cv2.circle(image1, (corner[1], corner[0]), 3, (0, 255, 0), -1)

cv2_imshow( image1)
cv2.waitKey(0)
cv2.destroyAllWindows()

"""g) Test and analyze the impact of:

i. Different values of the Gaussian window size (in part a)


ii. Different values of window size in part c.


iii. Different values of threshold Œ±. (part d)


ii. Different values of non-maximal suppression window size(w*w) in part e.


h) For matching the corner points between two images, we need to assign a descriptor to each feature point. There are different types of descriptors; some are just translation invariant, while some of them are both translation and rotation invariant and etc. One of the most recognized descriptors is MOPS, which is translation and rotation invariant, making it a good choice for many applications. We want to use a simplified MOPS algorithm to calculate descriptors for each corner. (you can read more about MOPS at: https://sbme-tutorials.github.io/2020/cv/presentations/week06_sift.html#5)

first Find the dominant orientation Œ∏ of the each keypoint. Each keypoint now is represented by (x,y,Œ∏)

Extract an 8√ó8 patch around the keypoint then Estimate the local mean and the std of intensities in the 8x8 and normaize the intensities. now each keypoint is represented by (x,y,Œ∏,Ihat)

Write a function based on the mentioned two steps. Lastly, use prebuilt OpenCV functions for the matching operation (output of MOPS function is input of opencv matching functions.)

Feel free to incorporate creativity and innovation in this part. There's a high chance that you may not achieve an ideal result because this is a basic and simplified descriptor
"""

def compute_orientation(grad_x, grad_y):
    theta = np.arctan2(grad_y, grad_x)
    return theta

# Compute the orientations for the keypoints
orientations1 = compute_orientation(grad_x1, grad_y1)
orientations2 = compute_orientation(grad_x2, grad_y2)


def extract_and_normalize_patch(image, keypoints, orientations, patch_size=8):
    half_patch = patch_size // 2
    descriptors = []

    for y, x in keypoints:
        # Extract the patch around the keypoint
        if y - half_patch < 0 or y + half_patch >= image.shape[0] or x - half_patch < 0 or x + half_patch >= image.shape[1]:
            continue  # Skip keypoints near the border

        patch = image[y-half_patch:y+half_patch, x-half_patch:x+half_patch]

        # Rotate the patch according to the dominant orientation
        theta = orientations[y, x]
        rotation_matrix = cv2.getRotationMatrix2D((half_patch, half_patch), np.degrees(theta), 1)
        rotated_patch = cv2.warpAffine(patch, rotation_matrix, (patch_size, patch_size))

        # Normalize the intensities
        mean = np.mean(rotated_patch)
        std = np.std(rotated_patch)
        if std > 0:
            normalized_patch = (rotated_patch - mean) / std
        else:
            normalized_patch = rotated_patch - mean

        descriptors.append((x, y, theta, normalized_patch))

    return descriptors

# Find keypoints in new_R (where R > 0)
keypoints1 = np.argwhere(new_R > 0)
keypoints2 = np.argwhere(new_R > 0)

# Compute descriptors for each keypoint
descriptors1 = extract_and_normalize_patch(gray_image1, keypoints1, orientations1)
descriptors2 = extract_and_normalize_patch(gray_image2, keypoints2, orientations2)

# Convert descriptors to a format suitable for BFMatcher
def descriptors_to_cv_format(descriptors):
    keypoints = []
    descriptors_list = []
    for (x, y, theta, patch) in descriptors:
        keypoints.append(cv2.KeyPoint(x, y, _size=8, _angle=np.degrees(theta)))
        descriptors_list.append(patch.flatten())
    return keypoints, np.array(descriptors_list, dtype=np.float32)

keypoints_cv1, descriptors_cv1 = descriptors_to_cv_format(descriptors1)
keypoints_cv2, descriptors_cv2 = descriptors_to_cv_format(descriptors2)

# Use BFMatcher to match the descriptors
bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)
matches = bf.match(descriptors_cv1, descriptors_cv2)

# Sort matches by distance
matches = sorted(matches, key=lambda x: x.distance)

# Draw matches
matched_image = cv2.drawMatches(image1, keypoints_cv1, image2, keypoints_cv2, matches[:10], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
cv2_imshow(matched_image)
cv2.waitKey(0)
cv2.destroyAllWindows()

"""2-SIFT descriptor

The SIFT descriptor is one of the most, or perhaps the most, widely applicable feature detection algorithms that find keypoints and assigns a descriptor to each keypoints. SIFT extracts feature points in a pyramid space created from a picture at different sizes and scales. (you can read mor about sift at: https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html)

Capture two images with your phone, ensuring that in the second image, your camera rotates +30 degrees and translates +15 cm (based on your assumption for positive directions and rotations)

dont capture the images from featureless environment such as a plain wall (if you can take a picture of chessboard)

a) find keypoints use sift prebuilt functions (dont forget the preprocess step!)
"""

import cv2
img = cv2.imread("/content/IMG_20240627_164714.jpg")
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
blured_img = cv2.GaussianBlur(gray,(3,3),0)
sift = cv2.SIFT_create()
keypoints, descriptors = sift.detectAndCompute(blured_img,None)

image_with_keypoints = cv2.drawKeypoints(blured_img,keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

cv2_imshow(image_with_keypoints)
cv2.waitKey(0)
cv2.destroyAllWindows()

"""b) Utilize prebuilt matching functions to find the matches between two images. Identify the best two candidates for each match and select the matching as a true match based on your arbitrary conditions. Visualize the matches between the two images. Adjust your conditions to find the best matching result visually."""

img1 = cv2.imread("/content/IMG_20240627_164714.jpg")
img2 = cv2.imread("/content//IMG_20240627_164738.jpg")
gray1 = cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY)

sift = cv2.SIFT_create()

keypoints1, descriptors1 = sift.detectAndCompute(gray1,None)
keypoints2, descriptors2 = sift.detectAndCompute(gray2,None)

bf =cv2.BFMatcher(cv2.NORM_L2,crossCheck=  True)
matches = bf.match(descriptors1,keypoints1,descriptors2)

matches = sorted(matches,key = lambda x:x.distance)

N = 50
img_matches = cv2.drawMatches(img1, keypoints1 , img2, keypoints2, matches[:N], None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)

cv2.imshow('SIFT image matching',img_matches)
cv2.waitKey(0)
cv2.destroyAllWindows()

"""c) as you can see in previous step there are stil some wrong matches. in this cases we can use ransac for detecting wrong matches or outliers. write a function for ransac manually. for this porpuse you need to use the fundamental matrix.  """

def ransac_fundamental_matrix(matches, kp1, kp2, threshold=1.0, iterations=1000):
    max_inliers = []
    final_fundamental_matrix = None

    for _ in range(iterations):
        # Randomly select 8 matches
        random_matches = np.random.choice(matches, 8, replace=False)

        # Extract the points from the matches
        pts1 = np.zeros((8, 2))
        pts2 = np.zeros((8, 2))
        for i, match in enumerate(random_matches):
            pts1[i] = kp1[match.queryIdx].pt
            pts2[i] = kp2[match.trainIdx].pt

        # Estimate the fundamental matrix
        F, mask = cv2.findFundamentalMat(pts1, pts2, method=cv2.FM_8POINT)

        if F is None or F.shape != (3, 3):
            continue

        # Compute inliers
        inliers = []
        for match in matches:
            pt1 = np.array([kp1[match.queryIdx].pt[0], kp1[match.queryIdx].pt[1], 1]).reshape(3, 1)
            pt2 = np.array([kp2[match.trainIdx].pt[0], kp2[match.trainIdx].pt[1], 1]).reshape(3, 1)

            # Sampson distance
            error = np.abs(np.dot(pt2.T, np.dot(F, pt1)))
            if error < threshold:
                inliers.append(match)

        # Update the final fundamental matrix if the current one has more inliers
        if len(inliers) > len(max_inliers):
            max_inliers = inliers
            final_fundamental_matrix = F

    return final_fundamental_matrix, max_inliers

# Convert keypoints to a list of points
pts1 = np.float32([keypoints_1[m.queryIdx].pt for m in matches])
pts2 = np.float32([keypoints_2[m.trainIdx].pt for m in matches])

# Apply RANSAC
F, inliers = ransac_fundamental_matrix(matches, keypoints_1, keypoints_2)

# Check if a valid fundamental matrix was found
if F is None:
    print("RANSAC failed to find a valid fundamental matrix.")
else:
    # Draw inlier matches
    inlier_matches = [matches[i] for i in range(len(matches)) if matches[i] in inliers]
    img_inliers = cv2.drawMatches(img1, keypoints_1, img2, keypoints_2, inlier_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
    plt.imshow(img_inliers), plt.show()

"""Use the final matches to calculate the fundamental matrix based on them. Then, compute the intrinsic matrix of your camera, assuming that ùëÜùúÉ is equal to zero. Ensure that your K matrix is logical."""

def compute_intrinsic_matrix(image_shape, focal_length):

    h, w = image_shape
    cx, cy = w / 2, h / 2

    K = np.array([[focal_length, 0, cx],
                  [0, focal_length, cy],
                  [0, 0, 1]])

    return K


focal_length = 1

# Compute intrinsic matrix
K = compute_intrinsic_matrix(img1.shape, focal_length)
print("Intrinsic Matrix:\n", K)